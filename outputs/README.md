# Outputs Directory

This directory contains various outputs generated by the multi-agent research system.

## Directory Structure

### Screenshots (`screenshots/`)

The `screenshots/` folder contains visual documentation of the system:

- **Safety Violations**: 
  - `safety_violation_*.png` - Screenshots showing guardrails being triggered and how safety violations are displayed in the UI
  - These demonstrate the system's safety mechanisms in action, including input validation, output sanitization, and user-facing error messages

- **UI Examples**:
  - `ui_example*.png` - Screenshots showing the Streamlit web interface during a single query run
  - These illustrate the user experience, agent activity display, and system responses

### Conversation Outputs (`conversation_*.json` and `conversation_*.txt`)

Files prefixed with `conversation_` contain the full output and conversation history from a single query run using `example_autogen.py`.

- **Format**: `conversation_YYYYMMDD_HHMMSS.{json|txt}`
- **JSON files** (`.json`): Complete structured data including:
  - Query
  - Final response
  - Metadata (plan, research findings, sources, citations)
  - Full conversation history with all agent messages
- **Text files** (`.txt`): Human-readable format containing:
  - Query
  - Final response
  - Metadata
  - Note: Full conversation history is available in the corresponding JSON file

### Evaluation Outputs (`evaluation_*.json`, `evaluation_report_*.md`, `evaluation_summary_*.txt`)

Files prefixed with `evaluation_` contain results from evaluation runs on test prompts from `data/test_queries.json`.

- **Format**: `evaluation_YYYYMMDD_HHMMSS.{json|md|txt}`
- **JSON files** (`.json`): Detailed evaluation results including:
  - Scores for each test query across all criteria
  - Individual judge evaluations (multiple perspectives)
  - System responses for each query
  - Aggregated statistics and summaries
- **Markdown reports** (`.md`): Comprehensive human-readable evaluation report suitable for inclusion in technical write-ups, including:
  - Executive summary
  - Overall performance metrics
  - Scores by judge perspective and criterion
  - Best/worst performing queries
  - Detailed results for each query
  - Evaluation methodology
- **Summary files** (`.txt`): Quick reference summary with:
  - Total queries, success rate
  - Overall average scores
  - Scores by judge perspective and criterion



